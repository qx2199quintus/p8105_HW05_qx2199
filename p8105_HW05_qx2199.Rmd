---
title: "p8105_HW05_qx2199"
output: html_document
author: Qianhui Xu
---



```{r setup, include = FALSE}
library(tidyverse)
library(purrr)
library(patchwork)
knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(1)
```


## Problem 1

Read in the data.

```{r}
homicide_df = 
  read_csv("./homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```


Let's look at this a bit

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Try to iterate ........

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```



```{r}
problem_1 =
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  labs( title = "proportions of unsolved homicides", x = "city", y = "proportion of unsolved homicides" )

problem_1
```





## Problem 2

import and tidy dataset.


```{r}
problem_2_df = 
  tibble(files = list.files(path = "./data",full.names = TRUE)) %>% 
  mutate(file_contents = map(.x = files, ~read_csv(.x))) %>% 
  unnest(file_contents) %>% 
   pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observations"
  ) %>% 
  mutate(files = str_replace(files, ".csv$", "")) %>% 
  mutate(files = str_replace(files, "con", "control"),
         files = str_replace(files, "exp", "experimental")) %>% 
  separate(files, into = c("arms_old", "id"), sep = "\\_") %>%   
  separate(arms_old,into=c("dot","data","arm"), sep ="/") %>%
  select(-dot,-data) %>% 
  mutate(id = as.numeric(id), arm = as.factor(arm), week = as.numeric(week)) %>% 
  relocate(arm, id)

problem_2_df
```

##### Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
problem_2_plot=
problem_2_df %>%
  ggplot(aes(x = week, y = observations, group = interaction(id,arm),color=arm))  +scale_color_manual(values=c("#6495ED","#FF9966"))+
  geom_line() + labs(  title = "Observations on each subject in experimental and control group ",x = "week", y = "observations")
problem_2_plot
```

##### Comment on differences between groups.

We found that at week 1,the observed values in the control group and the experimental group are similar. During the following weeks we can see that  there's an increasing trend in the observed values in the experimental group, while in the control group,the observed value seems to vary between -2.5 and 5. There's not a increasing/decreasing trend, and the trend seems to be relatively stable for the observed values in the control group. We could see that the difference of the ovserved values in the two groups become larger after 8 weeks. The overall observed values in the experimental group is higher than that in the control group after 8 weeks following-up.

## Problem 3



```{r}
set.seed(1)
```

Create the function

```{r}
problem_3 = function(n = 30, mu, sigma = 5) 
  {
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
 sim_data %>% 
    summarize(
      mu_hat = mean(x),
      t.test(x, mu = 0, conf.level = 0.95) %>% 
      broom::tidy() %>% 
      select(p.value))
}

```

Run the function 5000 times, obtain the dataset of interest when mu=0
```{r}
output_mu = vector("list", 5000)
for (i in 1:5000) {
  output_mu[[i]] = problem_3(mu = 0) 
 }
 sim_results = bind_rows(output_mu)
```

Create the dataset which contains the values we want when mu ranges from 0 to 6

```{r}
sim_mu_final = 
  tibble( mu = 0:6) %>% 
  mutate(
    output_mu_other = map(.x = mu, ~rerun (5000, problem_3(mu = .x))),
    other_mu_df = map(output_mu_other, bind_rows)
  ) %>% 
  select(-output_mu_other) %>% 
  unnest(other_mu_df) 
```

```{r}
sim_mu_final %>% 
  mutate( mu = as.factor(mu))
```

##### Q3.1.1 Make a plot showing the proportion of times the null was rejected on the y axis and the true value of mean on the x axis

```{r}
plot_one  =
sim_mu_final  %>% 
  group_by(mu) %>%
  mutate(outcome = ifelse(p.value < 0.05, "reject", "not reject") )%>% 
  summarize(num_reject = sum ( outcome == "reject"),num_total= n()) %>% 
  mutate(rejection_proportion = num_reject / num_total ) %>% 
ggplot(aes(x = mu, y = rejection_proportion, fill = mu)) + geom_line() +labs(title = "Power of the test",x = "True value of mean",y = "Proportion of times the null was rejected")

plot_one

```


##### Q3.1.2 Describe the association between effect size and power.

From the plot, we could see that as the effect size increases, the proportion of times that the null was rejected also increases, which indicates that the power increases as our effect size increases. The power is relative high, which approximate to 1 when the true values of mean equal to 4,5,and 6 when the effect size is relatively large.



##### Q3.2 Make a plot showing the average estimate of μ hat  on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ hat  only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 



```{r }
plot_two =
  sim_mu_final  %>% 
  group_by(mu) %>% 
  summarize(average_mu_hat = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y = average_mu_hat)) +
  geom_point() + geom_line() +
  labs(
    title = "Average estimate for all samples",
    x = "True value of mean",
    y = "average estimate of mu hat"
  )

plot_three = 
  sim_mu_final  %>% 
  group_by(mu) %>% 
  filter(p.value < 0.05) %>% 
  summarize(average_mu_hat_three = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y = average_mu_hat_three)) + geom_point() + geom_line() +labs(
    title = "Average estimate for samples for which the null was rejected",
    x = "True value of mean",
    y = "Average estimate of mu hat"
  )


plot_two + plot_three
```

##### Q3.3 Is the sample average of μ hat  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

Firstly, from the left plot we could see that the estimated mean equals to the true value of mean,because the dataset is normal distributed and follows central limit theorem. 

From the plot two and plot three we could see that when μ, the true value of mean = 0,4,5,6, the the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ; 

* Specifically,when true value of mean = 0, the power is relatively low, and given that the values that we reject are always randomly distributed above or below 0, the estimated mean for these values will also approximate to 0. 

* Also,when true value of mean = 4,5,6, the powers are relatively high which approximate to 1 and nearly all the tests reject the null. Therefore, the average estimated mean will be similar or very approximate to the true mean value.

From the plot two and plot three we could also see that when μ, the true value of mean = 1,2,3 the the sample average of μ hat across tests for which the the null is rejected is higher than the true value of μ.

* Specifically,when true value of mean = 1,2,3, under this scenario, power is relatively higher than true value of mean = 0 but is still not high enough. Samples on the left of the distribution (smaller means) will not reject the null while most of the samples which is larger than estimated mean will reject the null hypothesis, the average estimated mean will be higher than the true mean value.